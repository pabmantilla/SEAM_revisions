{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d47c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "from urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# TensorFlow/Keras imports for model loading\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# SEAM imports\n",
    "import seam\n",
    "from seam import Compiler, Attributer, Clusterer, MetaExplainer, Identifier\n",
    "from seam.logomaker_batch.batch_logo import BatchLogo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2df93cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 8\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#open libraries from library_selection.ipynb\n",
    "import pickle\n",
    "\n",
    "with open('../libraries/hyperparam_libraries.pkl', 'rb') as f:\n",
    "    libraries = pickle.load(f)\n",
    "    dev_loci = libraries['dev']\n",
    "    hk_loci = libraries['hk']\n",
    "\n",
    "print(len(dev_loci), len(hk_loci))\n",
    "\n",
    "## import model\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model_dir = '../models/deepstarr'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_json_file = os.path.join(model_dir, 'deepstarr.model.json')\n",
    "model_weights_file = os.path.join(model_dir, 'deepstarr.model.h5')\n",
    "\n",
    "with open(model_json_file, 'r') as f:\n",
    "    model_json = f.read()\n",
    "\n",
    "model = model_from_json(model_json, custom_objects={'Functional': tf.keras.Model})\n",
    "model.load_weights(model_weights_file)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "## load deepSHAP\n",
    "def seam_deepshap(x_mut, task_index):\n",
    "    x_ref = x_mut\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from keras.models import model_from_json\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    # Configuration\n",
    "    attribution_method = 'deepshap'  # or 'gradientshap', 'integratedgradients', etc.\n",
    "    task_index = task_index  # 0 for Dev, 1 for Hk\n",
    "    gpu = 0  # GPU device number\n",
    "    save_data = True\n",
    "    save_path = './attributions'  # Where to save results\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Model paths\n",
    "    keras_model_json = 'models/deepstarr/deepstarr.model.json'\n",
    "    keras_model_weights = 'models/deepstarr/deepstarr.model.h5'\n",
    "\n",
    "    if attribution_method == 'deepshap':\n",
    "        try:\n",
    "            # Disable eager execution first\n",
    "            tf.compat.v1.disable_eager_execution()\n",
    "            tf.compat.v1.disable_v2_behavior()\n",
    "            print(\"TensorFlow eager execution disabled for DeepSHAP compatibility\")\n",
    "            \n",
    "            # Import SHAP to configure handlers\n",
    "            try:\n",
    "                import shap\n",
    "            except ImportError:\n",
    "                print(\"ERROR: SHAP package is not installed.\")\n",
    "                print(\"To install SHAP for DeepSHAP attribution, run:\")\n",
    "                print(\"pip install kundajelab-shap==1\")\n",
    "                raise ImportError(\"SHAP package required for DeepSHAP attribution\")\n",
    "            \n",
    "            # Handle AddV2 operation (element-wise addition) as a linear operation\n",
    "            shap.explainers.deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers.deep.deep_tf.passthrough\n",
    "\n",
    "            # Load the model after eager execution is disabled\n",
    "            keras_model = model_from_json(open(keras_model_json).read(), custom_objects={'Functional': tf.keras.Model})\n",
    "            np.random.seed(113)\n",
    "            random.seed(0)\n",
    "            keras_model.load_weights(keras_model_weights)\n",
    "            model = keras_model\n",
    "            \n",
    "            # Rebuild model to ensure proper graph construction\n",
    "            _ = model(tf.keras.Input(shape=model.input_shape[1:]))\n",
    "            \n",
    "        except ImportError:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not setup TensorFlow for DeepSHAP. Error: {e}\")\n",
    "            print(\"DeepSHAP may not work properly.\")\n",
    "        \n",
    "        # Create attributer for DeepSHAP\n",
    "        def deepstarr_compress(x):\n",
    "            \"\"\"DeepSTARR compression function for DeepSHAP.\"\"\"\n",
    "            if hasattr(x, 'outputs'):\n",
    "                return tf.reduce_sum(x.outputs[task_index], axis=-1)\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "        attributer = Attributer(\n",
    "            model,\n",
    "            method=attribution_method,\n",
    "            task_index=task_index,\n",
    "            compress_fun=deepstarr_compress\n",
    "        )\n",
    "\n",
    "        attributer.show_params(attribution_method)\n",
    "\n",
    "        t1 = time.time()\n",
    "        attributions = attributer.compute(\n",
    "            x_ref=x_ref,\n",
    "            x=x_mut,\n",
    "            save_window=None,\n",
    "            batch_size=16,\n",
    "            gpu=gpu,\n",
    "        )\n",
    "        t2 = time.time() - t1\n",
    "        print(f'Attribution time: {t2/60:.2f} minutes')\n",
    "    else:\n",
    "        # Use unified Attributer for other methods\n",
    "        attributer = Attributer(\n",
    "            model,\n",
    "            method=attribution_method,\n",
    "            task_index=task_index,\n",
    "            compress_fun=lambda x: x,\n",
    "            pred_fun=model.predict_on_batch,\n",
    "        )\n",
    "\n",
    "        attributer.show_params(attribution_method)\n",
    "\n",
    "        t1 = time.time()\n",
    "        attributions = attributer.compute(\n",
    "            x_ref=x_ref,\n",
    "            x=x_mut,\n",
    "            save_window=None,\n",
    "            batch_size=256,\n",
    "            gpu=gpu\n",
    "        )\n",
    "        t2 = time.time() - t1\n",
    "        print(f'Attribution time: {t2/60:.2f} minutes')\n",
    "\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c0bcd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Save each sequence's library to a single HDF5 file\n",
    "def save_library(filepath, sequences, predictions, original_idx):\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        f.create_dataset('sequences', data=sequences, compression='gzip', compression_opts=4)\n",
    "        f.create_dataset('predictions', data=predictions, compression='gzip', compression_opts=4)\n",
    "        f.attrs['original_idx'] = original_idx\n",
    "        f.attrs['n_samples'] = len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c803628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28713.48it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 38.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 27201.95it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 40.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 27264.86it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 26750.51it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 27420.52it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 38.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28594.93it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.64it/s]\n"
     ]
    }
   ],
   "source": [
    "## Create 100K -> 1K libraries for each sequence: DEV\n",
    "import squid\n",
    "task_index = 0  # 0 for dev, 1 for hk\n",
    "x_seqs = dev_loci[\"ohe_seq\"]\n",
    "seq_indices = dev_loci[\"test_idx\"]  # Add this line\n",
    "print(x_seqs.shape)\n",
    "\n",
    "\n",
    "for i, (x_seq, idx) in enumerate(zip(x_seqs, seq_indices)):\n",
    "\n",
    "    x_seq = np.array(x_seq)\n",
    "\n",
    "    pred_generator = squid.predictor.ScalarPredictor(\n",
    "            pred_fun=model.predict_on_batch,\n",
    "            task_idx=task_index,\n",
    "            batch_size=512\n",
    "        )\n",
    "\n",
    "    mut_generator = squid.mutagenizer.RandomMutagenesis(\n",
    "        mut_rate=0.10,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    mave = squid.mave.InSilicoMAVE(\n",
    "            mut_generator,\n",
    "            pred_generator,\n",
    "            249,\n",
    "            mut_window=[0, 249]\n",
    "        )\n",
    "\n",
    "    x_mut, y_mut = mave.generate(x_seq, num_sim=100000)\n",
    "\n",
    "    save_library(f'mutagenisis_library/dev_seq_{idx}_100K.h5', x_mut, y_mut, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "120549d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 26964.20it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 26676.32it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28842.15it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 38.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 27937.60it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 27783.66it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28689.71it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 38.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 26400.77it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 41.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 29060.52it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 40.68it/s]\n"
     ]
    }
   ],
   "source": [
    "## Create 100K -> 1K libraries for each sequence: Hk\n",
    "import squid\n",
    "task_index = 1  # 0 for dev, 1 for hk\n",
    "x_seqs = hk_loci[\"ohe_seq\"]\n",
    "seq_indices = hk_loci[\"test_idx\"]  # Add this line\n",
    "print(x_seqs.shape)\n",
    "\n",
    "\n",
    "for i, (x_seq, idx) in enumerate(zip(x_seqs, seq_indices)):\n",
    "\n",
    "    x_seq = np.array(x_seq)\n",
    "\n",
    "    pred_generator = squid.predictor.ScalarPredictor(\n",
    "            pred_fun=model.predict_on_batch,\n",
    "            task_idx=task_index,\n",
    "            batch_size=512\n",
    "        )\n",
    "\n",
    "    mut_generator = squid.mutagenizer.RandomMutagenesis(\n",
    "        mut_rate=0.10,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    mave = squid.mave.InSilicoMAVE(\n",
    "            mut_generator,\n",
    "            pred_generator,\n",
    "            249,\n",
    "            mut_window=[0, 249]\n",
    "        )\n",
    "\n",
    "    x_mut, y_mut = mave.generate(x_seq, num_sim=100000)\n",
    "\n",
    "    save_library(f'mutagenisis_library/hk_seq_{idx}_100K.h5', x_mut, y_mut, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2837ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev seq_17977: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Dev seq_21916: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Dev seq_21289: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Dev seq_3881: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Dev seq_266: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Dev seq_22612: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Hk seq_31742: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Hk seq_12962: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Hk seq_12053: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Hk seq_24723: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Hk seq_12279: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Hk seq_20647: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Hk seq_4071: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "Hk seq_22627: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Define subset sizes\n",
    "subset_sizes = [75000, 50000, 25000, 10000, 5000, 1000]\n",
    "size_labels = ['75K', '50K', '25K', '10K', '5K', '1K']\n",
    "\n",
    "def load_full_library(filepath):\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        return f['sequences'][:], f['predictions'][:], f.attrs['original_idx']\n",
    "\n",
    "def save_subset(filepath, sequences, predictions, original_idx):\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        f.create_dataset('sequences', data=sequences, compression='gzip', compression_opts=4)\n",
    "        f.create_dataset('predictions', data=predictions, compression='gzip', compression_opts=4)\n",
    "        f.attrs['original_idx'] = original_idx\n",
    "        f.attrs['n_samples'] = len(sequences)\n",
    "\n",
    "# Process Dev libraries\n",
    "dev_indices = dev_loci[\"test_idx\"].tolist()\n",
    "for idx in dev_indices:\n",
    "    # Create directory for this sequence\n",
    "    seq_dir = f'mutagenisis_library/Dev/seq_{idx}'\n",
    "    os.makedirs(seq_dir, exist_ok=True)\n",
    "    \n",
    "    src_path = f'mutagenisis_library/dev_seq_{idx}_100K.h5'\n",
    "    seqs, preds, orig_idx = load_full_library(src_path)\n",
    "    \n",
    "    # Copy 100K directly (no subsetting)\n",
    "    save_subset(f'{seq_dir}/100K.h5', seqs, preds, orig_idx)\n",
    "    \n",
    "    # Create random subsets\n",
    "    indices_100k = np.arange(100000)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices_100k)\n",
    "    \n",
    "    for size, label in zip(subset_sizes, size_labels):\n",
    "        subset_idx = indices_100k[:size]\n",
    "        save_subset(f'{seq_dir}/{label}.h5', seqs[subset_idx], preds[subset_idx], orig_idx)\n",
    "    \n",
    "    print(f\"Dev seq_{idx}: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\")\n",
    "\n",
    "# Process Hk libraries\n",
    "hk_indices = hk_loci[\"test_idx\"].tolist()\n",
    "for idx in hk_indices:\n",
    "    seq_dir = f'mutagenisis_library/Hk/seq_{idx}'\n",
    "    os.makedirs(seq_dir, exist_ok=True)\n",
    "    \n",
    "    src_path = f'mutagenisis_library/hk_seq_{idx}_100K.h5'\n",
    "    seqs, preds, orig_idx = load_full_library(src_path)\n",
    "    \n",
    "    # Copy 100K directly (no subsetting)\n",
    "    save_subset(f'{seq_dir}/100K.h5', seqs, preds, orig_idx)\n",
    "    \n",
    "    # Create random subsets\n",
    "    indices_100k = np.arange(100000)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices_100k)\n",
    "    \n",
    "    for size, label in zip(subset_sizes, size_labels):\n",
    "        subset_idx = indices_100k[:size]\n",
    "        save_subset(f'{seq_dir}/{label}.h5', seqs[subset_idx], preds[subset_idx], orig_idx)\n",
    "    \n",
    "    print(f\"Hk seq_{idx}: all subsets created (100K, 75K, 50K, 25K, 10K, 5K, 1K)\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (SEAM)",
   "language": "python",
   "name": "seam_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
