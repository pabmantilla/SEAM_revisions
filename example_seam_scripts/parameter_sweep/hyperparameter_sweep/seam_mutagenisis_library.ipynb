{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d47c873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 14:53:37.548032: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-21 14:53:37.654802: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-21 14:53:37.658157: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:37.658175: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2026-01-21 14:53:38.064746: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:38.064856: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:38.064860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/grid/wsbs/home_norepl/pmantill/SEAM_revisions/SEAM_revisions/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "from urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# TensorFlow/Keras imports for model loading\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# SEAM imports\n",
    "import seam\n",
    "from seam import Compiler, Attributer, Clusterer, MetaExplainer, Identifier\n",
    "from seam.logomaker_batch.batch_logo import BatchLogo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2df93cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n",
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 14:53:40.473280: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:40.473401: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:40.473502: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:40.473592: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:40.473678: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:40.473765: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:40.473849: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:40.473937: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/grid/hpc/software/code-server/4.103.2-1/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/local/apps/python37/lib\n",
      "2026-01-21 14:53:40.473971: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2026-01-21 14:53:40.474922: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#open libraries from library_selection.ipynb\n",
    "import pickle\n",
    "\n",
    "with open('../libraries/hyperparam_libraries.pkl', 'rb') as f:\n",
    "    libraries = pickle.load(f)\n",
    "    dev_loci = libraries['dev']\n",
    "    hk_loci = libraries['hk']\n",
    "\n",
    "print(len(dev_loci), len(hk_loci))\n",
    "\n",
    "## import model\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model_dir = '../models/deepstarr'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_json_file = os.path.join(model_dir, 'deepstarr.model.json')\n",
    "model_weights_file = os.path.join(model_dir, 'deepstarr.model.h5')\n",
    "\n",
    "with open(model_json_file, 'r') as f:\n",
    "    model_json = f.read()\n",
    "\n",
    "model = model_from_json(model_json, custom_objects={'Functional': tf.keras.Model})\n",
    "model.load_weights(model_weights_file)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "## load deepSHAP\n",
    "def seam_deepshap(x_mut, task_index):\n",
    "    x_ref = x_mut\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from keras.models import model_from_json\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    # Configuration\n",
    "    attribution_method = 'deepshap'  # or 'gradientshap', 'integratedgradients', etc.\n",
    "    task_index = task_index  # 0 for Dev, 1 for Hk\n",
    "    gpu = 0  # GPU device number\n",
    "    save_data = True\n",
    "    save_path = './attributions'  # Where to save results\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Model paths\n",
    "    keras_model_json = 'models/deepstarr/deepstarr.model.json'\n",
    "    keras_model_weights = 'models/deepstarr/deepstarr.model.h5'\n",
    "\n",
    "    if attribution_method == 'deepshap':\n",
    "        try:\n",
    "            # Disable eager execution first\n",
    "            tf.compat.v1.disable_eager_execution()\n",
    "            tf.compat.v1.disable_v2_behavior()\n",
    "            print(\"TensorFlow eager execution disabled for DeepSHAP compatibility\")\n",
    "            \n",
    "            # Import SHAP to configure handlers\n",
    "            try:\n",
    "                import shap\n",
    "            except ImportError:\n",
    "                print(\"ERROR: SHAP package is not installed.\")\n",
    "                print(\"To install SHAP for DeepSHAP attribution, run:\")\n",
    "                print(\"pip install kundajelab-shap==1\")\n",
    "                raise ImportError(\"SHAP package required for DeepSHAP attribution\")\n",
    "            \n",
    "            # Handle AddV2 operation (element-wise addition) as a linear operation\n",
    "            shap.explainers.deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers.deep.deep_tf.passthrough\n",
    "\n",
    "            # Load the model after eager execution is disabled\n",
    "            keras_model = model_from_json(open(keras_model_json).read(), custom_objects={'Functional': tf.keras.Model})\n",
    "            np.random.seed(113)\n",
    "            random.seed(0)\n",
    "            keras_model.load_weights(keras_model_weights)\n",
    "            model = keras_model\n",
    "            \n",
    "            # Rebuild model to ensure proper graph construction\n",
    "            _ = model(tf.keras.Input(shape=model.input_shape[1:]))\n",
    "            \n",
    "        except ImportError:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not setup TensorFlow for DeepSHAP. Error: {e}\")\n",
    "            print(\"DeepSHAP may not work properly.\")\n",
    "        \n",
    "        # Create attributer for DeepSHAP\n",
    "        def deepstarr_compress(x):\n",
    "            \"\"\"DeepSTARR compression function for DeepSHAP.\"\"\"\n",
    "            if hasattr(x, 'outputs'):\n",
    "                return tf.reduce_sum(x.outputs[task_index], axis=-1)\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "        attributer = Attributer(\n",
    "            model,\n",
    "            method=attribution_method,\n",
    "            task_index=task_index,\n",
    "            compress_fun=deepstarr_compress\n",
    "        )\n",
    "\n",
    "        attributer.show_params(attribution_method)\n",
    "\n",
    "        t1 = time.time()\n",
    "        attributions = attributer.compute(\n",
    "            x_ref=x_ref,\n",
    "            x=x_mut,\n",
    "            save_window=None,\n",
    "            batch_size=16,\n",
    "            gpu=gpu,\n",
    "        )\n",
    "        t2 = time.time() - t1\n",
    "        print(f'Attribution time: {t2/60:.2f} minutes')\n",
    "    else:\n",
    "        # Use unified Attributer for other methods\n",
    "        attributer = Attributer(\n",
    "            model,\n",
    "            method=attribution_method,\n",
    "            task_index=task_index,\n",
    "            compress_fun=lambda x: x,\n",
    "            pred_fun=model.predict_on_batch,\n",
    "        )\n",
    "\n",
    "        attributer.show_params(attribution_method)\n",
    "\n",
    "        t1 = time.time()\n",
    "        attributions = attributer.compute(\n",
    "            x_ref=x_ref,\n",
    "            x=x_mut,\n",
    "            save_window=None,\n",
    "            batch_size=256,\n",
    "            gpu=gpu\n",
    "        )\n",
    "        t2 = time.time() - t1\n",
    "        print(f'Attribution time: {t2/60:.2f} minutes')\n",
    "\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0bcd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Save each sequence's library to a single HDF5 file\n",
    "def save_library(filepath, sequences, predictions, original_idx):\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        f.create_dataset('sequences', data=sequences, compression='gzip', compression_opts=4)\n",
    "        f.create_dataset('predictions', data=predictions, compression='gzip', compression_opts=4)\n",
    "        f.attrs['original_idx'] = original_idx\n",
    "        f.attrs['n_samples'] = len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c803628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28908.43it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 34.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 27571.74it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 37.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 27043.06it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 38.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28020.46it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 38.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28209.15it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28046.35it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 38.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 27989.67it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 37.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 25684.39it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 38.85it/s]\n"
     ]
    }
   ],
   "source": [
    "## Create 100K -> 1K libraries for each sequence: DEV\n",
    "import squid\n",
    "task_index = 0  # 0 for dev, 1 for hk\n",
    "x_seqs = dev_loci[\"ohe_seq\"]\n",
    "seq_indices = dev_loci[\"test_idx\"]  # Add this line\n",
    "print(x_seqs.shape)\n",
    "\n",
    "\n",
    "for i, (x_seq, idx) in enumerate(zip(x_seqs, seq_indices)):\n",
    "\n",
    "    x_seq = np.array(x_seq)\n",
    "\n",
    "    pred_generator = squid.predictor.ScalarPredictor(\n",
    "            pred_fun=model.predict_on_batch,\n",
    "            task_idx=task_index,\n",
    "            batch_size=512\n",
    "        )\n",
    "\n",
    "    mut_generator = squid.mutagenizer.RandomMutagenesis(\n",
    "        mut_rate=0.10,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    mave = squid.mave.InSilicoMAVE(\n",
    "            mut_generator,\n",
    "            pred_generator,\n",
    "            249,\n",
    "            mut_window=[0, 249]\n",
    "        )\n",
    "\n",
    "    x_mut, y_mut = mave.generate(x_seq, num_sim=100000)\n",
    "\n",
    "    save_library(f'mutagenisis_library/dev_seq_{idx}_100K.h5', x_mut, y_mut, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120549d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28784.71it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 37.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 27126.95it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28478.17it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 37.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28891.91it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 37.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 27550.91it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:04<00:00, 24758.58it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:03<00:00, 28989.52it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:05<00:00, 37.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 100000/100000 [00:04<00:00, 23459.90it/s]\n",
      "Inference: 100%|██████████| 195/195 [00:04<00:00, 39.14it/s]\n"
     ]
    }
   ],
   "source": [
    "## Create 100K -> 1K libraries for each sequence: Hk\n",
    "import squid\n",
    "task_index = 1  # 0 for dev, 1 for hk\n",
    "x_seqs = hk_loci[\"ohe_seq\"]\n",
    "seq_indices = hk_loci[\"test_idx\"]  # Add this line\n",
    "print(x_seqs.shape)\n",
    "\n",
    "\n",
    "for i, (x_seq, idx) in enumerate(zip(x_seqs, seq_indices)):\n",
    "\n",
    "    x_seq = np.array(x_seq)\n",
    "\n",
    "    pred_generator = squid.predictor.ScalarPredictor(\n",
    "            pred_fun=model.predict_on_batch,\n",
    "            task_idx=task_index,\n",
    "            batch_size=512\n",
    "        )\n",
    "\n",
    "    mut_generator = squid.mutagenizer.RandomMutagenesis(\n",
    "        mut_rate=0.10,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    mave = squid.mave.InSilicoMAVE(\n",
    "            mut_generator,\n",
    "            pred_generator,\n",
    "            249,\n",
    "            mut_window=[0, 249]\n",
    "        )\n",
    "\n",
    "    x_mut, y_mut = mave.generate(x_seq, num_sim=100000)\n",
    "\n",
    "    save_library(f'mutagenisis_library/hk_seq_{idx}_100K.h5', x_mut, y_mut, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2837ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev seq_17977: all subsets created with subset_idx\n",
      "Dev seq_21916: all subsets created with subset_idx\n",
      "Dev seq_21289: all subsets created with subset_idx\n",
      "Dev seq_3881: all subsets created with subset_idx\n",
      "Dev seq_266: all subsets created with subset_idx\n",
      "Dev seq_22612: all subsets created with subset_idx\n",
      "Dev seq_21069: all subsets created with subset_idx\n",
      "Dev seq_13748: all subsets created with subset_idx\n",
      "Hk seq_31742: all subsets created with subset_idx\n",
      "Hk seq_12962: all subsets created with subset_idx\n",
      "Hk seq_12053: all subsets created with subset_idx\n",
      "Hk seq_24723: all subsets created with subset_idx\n",
      "Hk seq_12279: all subsets created with subset_idx\n",
      "Hk seq_20647: all subsets created with subset_idx\n",
      "Hk seq_4071: all subsets created with subset_idx\n",
      "Hk seq_22627: all subsets created with subset_idx\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Define subset sizes\n",
    "subset_sizes = [75000, 50000, 25000, 10000, 5000, 1000]\n",
    "size_labels = ['75K', '50K', '25K', '10K', '5K', '1K']\n",
    "\n",
    "def load_full_library(filepath):\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        return f['sequences'][:], f['predictions'][:], f.attrs['original_idx']\n",
    "\n",
    "def save_subset(filepath, sequences, predictions, original_idx, subset_idx=None):\n",
    "    with h5py.File(filepath, 'w') as f:  # 'w' mode overwrites\n",
    "        f.create_dataset('sequences', data=sequences, compression='gzip', compression_opts=4)\n",
    "        f.create_dataset('predictions', data=predictions, compression='gzip', compression_opts=4)\n",
    "        if subset_idx is not None:\n",
    "            f.create_dataset('subset_idx', data=subset_idx, compression='gzip', compression_opts=4)\n",
    "        f.attrs['original_idx'] = original_idx\n",
    "        f.attrs['n_samples'] = len(sequences)\n",
    "\n",
    "# Process Dev libraries\n",
    "dev_indices = dev_loci[\"test_idx\"].tolist()\n",
    "for idx in dev_indices:\n",
    "    seq_dir = f'mutagenisis_library/Dev/seq_{idx}'\n",
    "    os.makedirs(seq_dir, exist_ok=True)\n",
    "    \n",
    "    src_path = f'mutagenisis_library/dev_seq_{idx}_100K.h5'\n",
    "    seqs, preds, orig_idx = load_full_library(src_path)\n",
    "    \n",
    "    # Save 100K (no subset_idx needed)\n",
    "    save_subset(f'{seq_dir}/100K.h5', seqs, preds, orig_idx, subset_idx=None)\n",
    "    \n",
    "    # Create random subsets\n",
    "    indices_100k = np.arange(100000)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices_100k)\n",
    "    \n",
    "    for size, label in zip(subset_sizes, size_labels):\n",
    "        subset_idx = indices_100k[:size]\n",
    "        save_subset(f'{seq_dir}/{label}.h5', seqs[subset_idx], preds[subset_idx], orig_idx, subset_idx=subset_idx)\n",
    "    \n",
    "    print(f\"Dev seq_{idx}: all subsets created with subset_idx\")\n",
    "\n",
    "# Process Hk libraries\n",
    "hk_indices = hk_loci[\"test_idx\"].tolist()\n",
    "for idx in hk_indices:\n",
    "    seq_dir = f'mutagenisis_library/Hk/seq_{idx}'\n",
    "    os.makedirs(seq_dir, exist_ok=True)\n",
    "    \n",
    "    src_path = f'mutagenisis_library/hk_seq_{idx}_100K.h5'\n",
    "    seqs, preds, orig_idx = load_full_library(src_path)\n",
    "    \n",
    "    # Save 100K (no subset_idx needed)\n",
    "    save_subset(f'{seq_dir}/100K.h5', seqs, preds, orig_idx, subset_idx=None)\n",
    "    \n",
    "    # Create random subsets\n",
    "    indices_100k = np.arange(100000)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices_100k)\n",
    "    \n",
    "    for size, label in zip(subset_sizes, size_labels):\n",
    "        subset_idx = indices_100k[:size]\n",
    "        save_subset(f'{seq_dir}/{label}.h5', seqs[subset_idx], preds[subset_idx], orig_idx, subset_idx=subset_idx)\n",
    "    \n",
    "    print(f\"Hk seq_{idx}: all subsets created with subset_idx\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (SEAM)",
   "language": "python",
   "name": "seam_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
