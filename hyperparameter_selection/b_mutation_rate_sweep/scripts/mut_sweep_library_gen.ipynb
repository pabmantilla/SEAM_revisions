{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6b8caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# TensorFlow/Keras imports for model loading\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# SQUID imports for mutagenesis\n",
    "import squid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b851c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# load dev 20\n",
    "\n",
    "dev_path = \"/grid/wsbs/home_norepl/pmantill/SEAM_revisions/SEAM_revisions/hyperparameter_selection/library_creation/Dev_20_library/Dev_20\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#open pickle file\n",
    "path = os.path.join(dev_path, \"dev_20_library.pkl\")\n",
    "\n",
    "dev_pkl = pd.read_pickle(path)\n",
    "\n",
    "dev_pkl = dev_pkl[\"dev\"]\n",
    "\n",
    "#dev_pd = pd.DataFrame(dev_pkl, index=[\"test_idx\"])\n",
    "print(len(dev_pkl))\n",
    "#remove the removed seqs (21916, 1693, 8389)\n",
    "\n",
    "dev_pkl = dev_pkl[~dev_pkl[\"test_idx\"].isin([21916, 1693, 8389])]\n",
    "dev_pkl = dev_pkl.reset_index(drop=True)\n",
    "print(len(dev_pkl))\n",
    "#save the new dev_pkl in data and model dir\n",
    "\n",
    "save_path = \"/grid/wsbs/home_norepl/pmantill/SEAM_revisions/SEAM_revisions/hyperparameter_selection/data_and_models/dev_20_library/\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "with open(os.path.join(save_path, 'dev_20_library.pkl'), 'wb') as f:\n",
    "    pickle.dump({'dev': dev_pkl}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f6854a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test_idx                                                22612\n",
       "sequence    TTTTAATGACTGAAATTAAAACATCATTAAGGCGAATTGGCCACCG...\n",
       "activity                                             3.265582\n",
       "ohe_seq     [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load dev 20 pickle\n",
    "\n",
    "dev_20_path = \"/grid/wsbs/home_norepl/pmantill/SEAM_revisions/SEAM_revisions/hyperparameter_selection/data_and_models/dev_20_library/\"\n",
    "dev_pkl = pd.read_pickle(os.path.join(save_path, 'dev_20_library.pkl'))\n",
    "print(len(dev_pkl[\"dev\"]))\n",
    "dev_pkl = dev_pkl[\"dev\"]\n",
    "\n",
    "dev_pkl.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4be52a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing /grid/wsbs/home_norepl/pmantill/SEAM_revisions/SEAM_revisions/hyperparameter_selection/data_and_models/models/deepstarr.model.json\n",
      "Using existing /grid/wsbs/home_norepl/pmantill/SEAM_revisions/SEAM_revisions/hyperparameter_selection/data_and_models/models/deepstarr.model.h5\n",
      "\n",
      "Model loaded successfully!\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "\n",
      "Wild-type predictions: (3.2655823, 0.6504629)\n",
      "Model input shape: (None, 249, 4)\n",
      "Model output shape: [(None, 1), (None, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Download and load the DeepSTARR model\n",
    "model_dir = \"/grid/wsbs/home_norepl/pmantill/SEAM_revisions/SEAM_revisions/hyperparameter_selection/data_and_models/models/\"\n",
    "MODEL_DIR = model_dir\n",
    "\n",
    "# Download model files if not present\n",
    "model_json_file = os.path.join(model_dir, 'deepstarr.model.json')\n",
    "model_weights_file = os.path.join(model_dir, 'deepstarr.model.h5')\n",
    "\n",
    "if not os.path.exists(model_json_file):\n",
    "    print(\"Downloading deepstarr.model.json...\")\n",
    "    url = 'https://www.dropbox.com/scl/fi/y1mwsqpv2e514md9t68jz/deepstarr.model.json?rlkey=cdwhstqf96fibshes2aov6t1e&st=9a0c5skz&dl=1'\n",
    "    urlretrieve(url, model_json_file)\n",
    "else:\n",
    "    print(f\"Using existing {model_json_file}\")\n",
    "\n",
    "if not os.path.exists(model_weights_file):\n",
    "    print(\"Downloading deepstarr.model.h5...\")\n",
    "    url = 'https://www.dropbox.com/scl/fi/6nl6e2hofyw70lh99h3uk/deepstarr.model.h5?rlkey=hqfnivn199xa54bjh8dn2jpaf&st=l4jig4ky&dl=1'\n",
    "    urlretrieve(url, model_weights_file)\n",
    "else:\n",
    "    print(f\"Using existing {model_weights_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the model architecture from JSON\n",
    "with open(model_json_file, 'r') as f:\n",
    "    model_json = f.read()\n",
    "\n",
    "model = model_from_json(model_json, custom_objects={'Functional': tf.keras.Model})\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(113)\n",
    "random.seed(0)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_weights(model_weights_file)\n",
    "num_tasks = 2  # Dev [0] and Hk [1]\n",
    "\n",
    "alphabet = ['A','C','G','T']\n",
    "\n",
    "x_ref = dev_pkl.iloc[0][\"ohe_seq\"]\n",
    "x_ref = np.expand_dims(x_ref,0)\n",
    "\n",
    "\n",
    "# Define mutagenesis window for sequence\n",
    "seq_length = x_ref.shape[1]\n",
    "mut_window = [0, seq_length]  # [start_position, stop_position]\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "\n",
    "# Forward pass to get output for the specific head\n",
    "output = model(x_ref)\n",
    "predd,predh = model.predict(x_ref)[0], model.predict(x_ref)[1]\n",
    "print(f\"\\nWild-type predictions: {predd[0][0], predh[0][0]}\")\n",
    "print(f\"Model input shape: {model.input_shape}\")\n",
    "print(f\"Model output shape: {model.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01440ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to save library to HDF5\n",
    "def save_library(filepath, sequences, predictions, original_idx):\n",
    "    \"\"\"Save mutagenesis library to HDF5 file.\"\"\"\n",
    "    n_samples = len(sequences)\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        f.create_dataset('sequences', data=sequences, compression='gzip', compression_opts=4)\n",
    "        f.create_dataset('predictions', data=predictions, compression='gzip', compression_opts=4)\n",
    "        # Add library_index for consistent subsetting (0 to n_samples-1)\n",
    "        f.create_dataset('library_index', data=np.arange(n_samples), compression='gzip', compression_opts=4)\n",
    "        f.attrs['original_idx'] = original_idx\n",
    "        f.attrs['n_samples'] = n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bcb71790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "## DeepSHAP attribution function with checkpointing\n",
    "def seam_deepshap(x_mut, task_index, checkpoint_path=None, checkpoint_every=5000):\n",
    "    \"\"\"Compute DeepSHAP attributions with optional checkpointing.\"\"\"\n",
    "    x_ref = x_mut\n",
    "    print(f\"Computing attributions for task_index: {task_index}\")\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from keras.models import model_from_json\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    # Check for existing checkpoint\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        with h5py.File(checkpoint_path, 'r') as f:\n",
    "            start_idx = f.attrs['last_completed_idx'] + 1\n",
    "            attributions_partial = f['attributions'][:start_idx]\n",
    "        print(f\"Resuming from checkpoint at index {start_idx}\")\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        attributions_partial = None\n",
    "\n",
    "    # If already complete, return\n",
    "    if start_idx >= len(x_mut):\n",
    "        print(\"Attributions already complete, loading from checkpoint\")\n",
    "        with h5py.File(checkpoint_path, 'r') as f:\n",
    "            return f['attributions'][:]\n",
    "\n",
    "    # Configuration\n",
    "    attribution_method = 'deepshap'\n",
    "    gpu = 0\n",
    "    \n",
    "    # Model paths\n",
    "    keras_model_json = os.path.join(MODEL_DIR, 'deepstarr.model.json')\n",
    "    keras_model_weights = os.path.join(MODEL_DIR, 'deepstarr.model.h5')\n",
    "\n",
    "    if attribution_method == 'deepshap':\n",
    "        try:\n",
    "            tf.compat.v1.disable_eager_execution()\n",
    "            tf.compat.v1.disable_v2_behavior()\n",
    "            print(\"TensorFlow eager execution disabled for DeepSHAP compatibility\")\n",
    "            \n",
    "            try:\n",
    "                import shap\n",
    "            except ImportError:\n",
    "                raise ImportError(\"SHAP package required for DeepSHAP attribution\")\n",
    "            \n",
    "            shap.explainers.deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers.deep.deep_tf.passthrough\n",
    "\n",
    "            keras_model = model_from_json(open(keras_model_json).read(), custom_objects={'Functional': tf.keras.Model})\n",
    "            np.random.seed(113)\n",
    "            random.seed(0)\n",
    "            keras_model.load_weights(keras_model_weights)\n",
    "            model_local = keras_model\n",
    "            \n",
    "            _ = model_local(tf.keras.Input(shape=model_local.input_shape[1:]))\n",
    "            \n",
    "        except ImportError:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not setup TensorFlow for DeepSHAP. Error: {e}\")\n",
    "            print(\"DeepSHAP may not work properly.\")\n",
    "        \n",
    "        def deepstarr_compress(x):\n",
    "            if hasattr(x, 'outputs'):\n",
    "                return tf.reduce_sum(x.outputs[task_index], axis=-1)\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "        attributer = Attributer(\n",
    "            model_local,\n",
    "            method=attribution_method,\n",
    "            task_index=task_index,\n",
    "            compress_fun=deepstarr_compress\n",
    "        )\n",
    "\n",
    "        attributer.show_params(attribution_method)\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        # Process in chunks with checkpointing\n",
    "        n_samples = len(x_mut)\n",
    "        all_attributions = []\n",
    "        \n",
    "        # Add previously computed attributions if resuming\n",
    "        if attributions_partial is not None:\n",
    "            all_attributions.append(attributions_partial)\n",
    "        \n",
    "        for chunk_start in range(start_idx, n_samples, checkpoint_every):\n",
    "            chunk_end = min(chunk_start + checkpoint_every, n_samples)\n",
    "            print(f\"\\nProcessing samples {chunk_start} to {chunk_end} of {n_samples}\")\n",
    "            \n",
    "            x_chunk = x_mut[chunk_start:chunk_end]\n",
    "            x_ref_chunk = x_chunk\n",
    "            \n",
    "            chunk_attributions = attributer.compute(\n",
    "                x_ref=x_ref_chunk,\n",
    "                x=x_chunk,\n",
    "                save_window=None,\n",
    "                batch_size=64,\n",
    "                gpu=gpu,\n",
    "            )\n",
    "            \n",
    "            all_attributions.append(chunk_attributions)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if checkpoint_path:\n",
    "                attributions_so_far = np.concatenate(all_attributions, axis=0)\n",
    "                os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "                with h5py.File(checkpoint_path, 'w') as f:\n",
    "                    f.create_dataset('attributions', data=attributions_so_far, compression='gzip', compression_opts=4)\n",
    "                    f.attrs['last_completed_idx'] = chunk_end - 1\n",
    "                    f.attrs['n_samples'] = n_samples\n",
    "                print(f\"Checkpoint saved at index {chunk_end - 1}\")\n",
    "        \n",
    "        attributions = np.concatenate(all_attributions, axis=0)\n",
    "        \n",
    "        t2 = time.time() - t1\n",
    "        print(f'Attribution time: {t2/60:.2f} minutes')\n",
    "        \n",
    "        return attributions\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "def load_library_25k(seq_idx):\n",
    "    \"\"\"Load the full 100K library for a Dev_20 sequence.\"\"\"\n",
    "    filepath = f'/grid/wsbs/home_norepl/pmantill/SEAM_revisions/SEAM_revisions/hyperparameter_selection/b_mutation_rate_sweep/seq_libraries/mut_sweep/Dev/seq_{seq_idx}/25K.h5'\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        sequences = f['sequences'][:]\n",
    "        predictions = f['predictions'][:]\n",
    "        original_idx = f.attrs['original_idx']\n",
    "        library_index = f['library_index'][:] if 'library_index' in f else np.arange(len(sequences))\n",
    "    return sequences, predictions, original_idx, library_index\n",
    "\n",
    "\n",
    "def create_subset_indices(library_index, subset_size, seed=42):\n",
    "    \"\"\"Create subset indices by shuffling library_index with a fixed seed.\"\"\"\n",
    "    indices = library_index.copy()\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indices)\n",
    "    return indices[:subset_size]\n",
    "\n",
    "\n",
    "def save_attributions(filepath, attributions, original_idx, subset_idx=None):\n",
    "    \"\"\"Save attributions to HDF5 file with optional subset indices.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        f.create_dataset('attributions', data=attributions, compression='gzip', compression_opts=4)\n",
    "        if subset_idx is not None:\n",
    "            f.create_dataset('subset_idx', data=subset_idx, compression='gzip', compression_opts=4)\n",
    "        f.attrs['original_idx'] = original_idx\n",
    "        f.attrs['n_samples'] = len(attributions)\n",
    "\n",
    "\n",
    "def load_attributions(filepath):\n",
    "    \"\"\"Load attributions from HDF5 file.\"\"\"\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        return f['attributions'][:]\n",
    "\n",
    "\n",
    "def attributions_exist(seq_idx):\n",
    "    \"\"\"Check if 100K attributions already exist for a sequence.\"\"\"\n",
    "    filepath = f'{RESULTS_DIR}/attribution_maps/deepSHAP/Dev/seq_{seq_idx}/100K.h5'\n",
    "    return os.path.exists(filepath)\n",
    "\n",
    "\n",
    "def all_attributions_exist(seq_idx):\n",
    "    \"\"\"Check if ALL attribution files exist for a given sequence.\"\"\"\n",
    "    for size_label in subset_sizes.keys():\n",
    "        attr_path = f'{RESULTS_DIR}/attribution_maps/deepSHAP/Dev/seq_{seq_idx}/{size_label}.h5'\n",
    "        if not os.path.exists(attr_path):\n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317f734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 25000/25000 [00:02<00:00, 12433.97it/s]\n",
      "Inference: 100%|██████████| 48/48 [00:01<00:00, 27.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20] Created seq_22612/25K.h5 with Mutation Rate 75.0%\n",
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 25000/25000 [00:01<00:00, 12593.21it/s]\n",
      "Inference: 100%|██████████| 48/48 [00:01<00:00, 27.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/20] Created seq_21069/25K.h5 with Mutation Rate 75.0%\n",
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 25000/25000 [00:02<00:00, 11455.86it/s]\n",
      "Inference: 100%|██████████| 48/48 [00:01<00:00, 29.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/20] Created seq_13748/25K.h5 with Mutation Rate 75.0%\n",
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 25000/25000 [00:02<00:00, 12451.55it/s]\n",
      "Inference: 100%|██████████| 48/48 [00:01<00:00, 29.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/20] Created seq_3881/25K.h5 with Mutation Rate 75.0%\n",
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis: 100%|██████████| 25000/25000 [00:02<00:00, 12305.06it/s]\n",
      "Inference: 100%|██████████| 48/48 [00:01<00:00, 29.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/20] Created seq_2974/25K.h5 with Mutation Rate 75.0%\n",
      "\n",
      "Building in silico MAVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis:  87%|████████▋ | 21807/25000 [00:01<00:00, 11503.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 52\u001b[0m\n\u001b[1;32m     44\u001b[0m mave \u001b[38;5;241m=\u001b[39m squid\u001b[38;5;241m.\u001b[39mmave\u001b[38;5;241m.\u001b[39mInSilicoMAVE(\n\u001b[1;32m     45\u001b[0m     mut_generator,\n\u001b[1;32m     46\u001b[0m     pred_generator,\n\u001b[1;32m     47\u001b[0m     seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m249\u001b[39m,\n\u001b[1;32m     48\u001b[0m     mut_window\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m249\u001b[39m]\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Generate 25k mutant sequences\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m x_mut, y_mut \u001b[38;5;241m=\u001b[39m \u001b[43mmave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlib_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# save each in \u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# add subset_idx col to ../seq_libraries/Seq_X/10%/25K.h5\u001b[39;00m\n\u001b[1;32m     57\u001b[0m subset_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(lib_size)\n",
      "File \u001b[0;32m~/SEAM_revisions/SEAM_revisions/.venv/lib/python3.8/site-packages/squid/mave.py:86\u001b[0m, in \u001b[0;36mInSilicoMAVE.generate\u001b[0;34m(self, x, num_sim, seed, verbose)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmut_window \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     x_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimit_range(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_position, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_position)\n\u001b[0;32m---> 86\u001b[0m     x_mut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmut_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_agnostic:\n\u001b[1;32m     88\u001b[0m         x_mut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_seq_random(x_mut, x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_position, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_position)\n",
      "File \u001b[0;32m~/SEAM_revisions/SEAM_revisions/.venv/lib/python3.8/site-packages/squid/mutagenizer.py:69\u001b[0m, in \u001b[0;36mRandomMutagenesis.__call__\u001b[0;34m(self, x, num_sim)\u001b[0m\n\u001b[1;32m     67\u001b[0m     num_muts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpoisson(avg_num_mut, (num_sim, \u001b[38;5;241m1\u001b[39m))[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     68\u001b[0m num_muts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(num_muts, \u001b[38;5;241m0\u001b[39m, L)\n\u001b[0;32m---> 69\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m \u001b[43mapply_mut_by_seq_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_muts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m one_hot\n",
      "File \u001b[0;32m~/SEAM_revisions/SEAM_revisions/.venv/lib/python3.8/site-packages/squid/mutagenizer.py:336\u001b[0m, in \u001b[0;36mapply_mut_by_seq_index\u001b[0;34m(x_index, shape, num_muts)\u001b[0m\n\u001b[1;32m    334\u001b[0m seq_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(x_index)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(mut_index, mut):\n\u001b[0;32m--> 336\u001b[0m     \u001b[43mseq_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# wrap non-sensical indices back to alphabet -- effectively makes it random mutation\u001b[39;00m\n\u001b[1;32m    339\u001b[0m seq_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmod(seq_index, A)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate 25K mutagenesis libraries for each Dev_20 sequence \n",
    "# and sweep through mutation rates\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "mutation_rates = [.75, .50, .25, .10, .5, .1]  \n",
    "lib_size = 25000\n",
    "\n",
    "\n",
    "for mut_rate in mutation_rates:\n",
    "\n",
    "    task_index = 0  # 0 for Dev\n",
    "    x_seqs = dev_pkl[\"ohe_seq\"]\n",
    "    seq_indices = dev_pkl[\"test_idx\"]\n",
    "\n",
    "    for i, (x_seq, idx) in enumerate(zip(x_seqs, seq_indices)):\n",
    "        output_dir = f'/grid/wsbs/home_norepl/pmantill/SEAM_revisions/SEAM_revisions/hyperparameter_selection/b_mutation_rate_sweep/seq_libraries/mut_sweep/Dev/seq_{idx}/{mut_rate*100}%/'\n",
    "        output_file = f'{output_dir}/25K.h5'\n",
    "        \n",
    "        # Check if library already exists\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping seq_{idx} - already exists\")\n",
    "            continue\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        x_seq = np.array(x_seq)\n",
    "        \n",
    "        # Create predictor\n",
    "        pred_generator = squid.predictor.ScalarPredictor(\n",
    "            pred_fun=model.predict_on_batch,\n",
    "            task_idx=task_index,\n",
    "            batch_size=512\n",
    "        )\n",
    "        \n",
    "        # Create mutagenizer\n",
    "        mut_generator = squid.mutagenizer.RandomMutagenesis(\n",
    "            mut_rate=mut_rate,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Create MAVE\n",
    "        mave = squid.mave.InSilicoMAVE(\n",
    "            mut_generator,\n",
    "            pred_generator,\n",
    "            seq_length = 249,\n",
    "            mut_window=[0, 249]\n",
    "        )\n",
    "        \n",
    "        # Generate 25k mutant sequences\n",
    "        x_mut, y_mut = mave.generate(x_seq, num_sim=lib_size)\n",
    "        \n",
    "        # save each in \n",
    "\n",
    "        # add subset_idx col to ../seq_libraries/Seq_X/10%/25K.h5\n",
    "        subset_idx = np.arange(lib_size)\n",
    "        save_library(output_file, x_mut, y_mut, idx)\n",
    "        print(f\"[{i+1}/{len(x_seqs)}] Created seq_{idx}/25K.h5 with Mutation Rate {mut_rate*100}%\")\n",
    "\n",
    "    # get deepshap attributions for each library\n",
    "    for idx in seq_indices:\n",
    "        output_dir = f'/grid/wsbs/home_norepl/pmantill/SEAM_revisions/SEAM_revisions/hyperparameter_selection/b_mutation_rate_sweep/seq_libraries/mut_sweep/deepshap/Dev/seq_{idx}/{mut_rate*100}%/'\n",
    "        output_file = f'{output_dir}/25K.h5'\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping seq_{idx} - already exists\")\n",
    "            continue\n",
    "        \n",
    "        # load library\n",
    "        x_mut, y_mut, original_idx, library_index = load_library_25k(idx)\n",
    "        \n",
    "        # get deepshap attributions\n",
    "        attributions = seam_deepshap(x_mut, task_index, checkpoint_path=output_file)\n",
    "\n",
    "        # save attributions\n",
    "        save_attributions(output_file, attributions, original_idx)\n",
    "        \n",
    "        print(f\"Saved attributions for seq_{idx} with Mutation Rate {mut_rate*100}%\")\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
